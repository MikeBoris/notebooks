{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import cx_Oracle\n",
    "from IPython.core.display import display, HTML\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# db cnx\n",
    "connection = cx_Oracle.connect('#############')\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<blockquote>('A social network analysis of substance use among immigrant adolescents in six European cities.',)</blockquote>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cat = \"118926776\"\n",
    "query = \"SELECT MFS.ARTICLE.ARTTITLE FROM MFS.ARTICLE WHERE MFS.ARTICLE.AN = '%s'\" % (cat)\n",
    "ti_lob = cursor.execute(query)\n",
    "result_ti = cursor.fetchone()\n",
    "display(HTML(\"<blockquote>{}</blockquote>\".format(result_ti)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to read LOB\n",
    "def read_LOB(cursor_execute):\n",
    "\tfor row in cursor:\n",
    "\t\tprint row[0].read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Background Social integration and the health of adolescents with a migration background is a major concern in multicultural societies. The literature, however, has paid little attention to the wider determinants of their health behaviours, including the composition of their social networks. The aim of this study was to describe the composition of adolescents&#8217; social networks according to migration background, and to examine how social networks are associated with substance use. Method In 2013, the SILNE study surveyed 11,015 secondary-school adolescents in 50 schools in six European cities in Belgium, Finland, Germany, Italy, the Netherlands, and Portugal, using a social network design. Each adolescent nominated up to five of their best and closest friends. Migration status was defined as first-generation migrants, second-generation migrants, and speaking another language at home. We computed two groups of network structural positions, the centrality of individual adolescents in networks, and the homophily of their social ties regarding migration (same-migration). Multilevel logistic regression was used to model the association between network structural position and smoking, alcohol use, and cannabis use. Results Compared with non-migrant adolescents, adolescents with migration backgrounds had similar relationship patterns. But almost half their social ties were with same-migration-background adolescents; non-migrants had few social ties to migrants. For adolescents with a migration background, a higher proportion of social ties with non-migrants was associated with increased use of cannabis (OR = 1.07, p = 0.03) and alcohol (OR = 1.08, p &#60; 0.01), but not with increased smoking ( p = 0.60). Popular migrant adolescents were at less risk of smoking, alcohol use, and cannabis use than popular non-migrant adolescents. Conclusion Homophily of social ties by migration background is noticeable in European schools. The tendency of migrant adolescents to have same-migration social ties may isolate them from non-migrant adolescents, but also reduces their risky health behaviours, in particular cannabis and alcohol use.\n"
     ]
    }
   ],
   "source": [
    "# retrieve abstract\n",
    "query1 = \"SELECT MFS.ARTABX.ABSTRACT FROM MFS.ARTABX WHERE MFS.ARTABX.AN = '%s'\" % (cat)\n",
    "abs_lob = cursor.execute(query1)\n",
    "result_abs = read_LOB(abs_lob)\n",
    "\n",
    "# close db cnx\n",
    "cursor.close()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  ('A social network analysis of substance use among immigrant adolescents in six European cities.',)\n",
      "Abstract: "
     ]
    },
    {
     "ename": "InterfaceError",
     "evalue": "not open",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInterfaceError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-abe77b47470a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Title: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_ti\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Abstract: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mread_LOB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs_lob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-f546898bcec1>\u001b[0m in \u001b[0;36mread_LOB\u001b[0;34m(cursor_execute)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# function to read LOB\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_LOB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcursor_execute\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m                 \u001b[1;32mprint\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInterfaceError\u001b[0m: not open"
     ]
    }
   ],
   "source": [
    "print \"Title: \", result_ti\n",
    "print \"Abstract: \", read_LOB(abs_lob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "abs = \"Background Social integration and the health of adolescents with a migration background is a major concern in multicultural societies. The literature, however, has paid little attention to the wider determinants of their health behaviours, including the composition of their social networks. The aim of this study was to describe the composition of adolescents&#8217; social networks according to migration background, and to examine how social networks are associated with substance use. Method In 2013, the SILNE study surveyed 11,015 secondary-school adolescents in 50 schools in six European cities in Belgium, Finland, Germany, Italy, the Netherlands, and Portugal, using a social network design. Each adolescent nominated up to five of their best and closest friends. Migration status was defined as first-generation migrants, second-generation migrants, and speaking another language at home. We computed two groups of network structural positions, the centrality of individual adolescents in networks, and the homophily of their social ties regarding migration (same-migration). Multilevel logistic regression was used to model the association between network structural position and smoking, alcohol use, and cannabis use. Results Compared with non-migrant adolescents, adolescents with migration backgrounds had similar relationship patterns. But almost half their social ties were with same-migration-background adolescents; non-migrants had few social ties to migrants. For adolescents with a migration background, a higher proportion of social ties with non-migrants was associated with increased use of cannabis (OR = 1.07, p = 0.03) and alcohol (OR = 1.08, p &#60; 0.01), but not with increased smoking ( p = 0.60). Popular migrant adolescents were at less risk of smoking, alcohol use, and cannabis use than popular non-migrant adolescents. Conclusion Homophily of social ties by migration background is noticeable in European schools. The tendency of migrant adolescents to have same-migration social ties may isolate them from non-migrant adolescents, but also reduces their risky health behaviours, in particular cannabis and alcohol use.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ti = \"A social network analysis of substance use among immigrant adolescents in six European cities.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<blockquote>A social network analysis of substance use among immigrant adolescents in six European cities.</blockquote>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<blockquote>Background Social integration and the health of adolescents with a migration background is a major concern in multicultural societies. The literature, however, has paid little attention to the wider determinants of their health behaviours, including the composition of their social networks. The aim of this study was to describe the composition of adolescents&#8217; social networks according to migration background, and to examine how social networks are associated with substance use. Method In 2013, the SILNE study surveyed 11,015 secondary-school adolescents in 50 schools in six European cities in Belgium, Finland, Germany, Italy, the Netherlands, and Portugal, using a social network design. Each adolescent nominated up to five of their best and closest friends. Migration status was defined as first-generation migrants, second-generation migrants, and speaking another language at home. We computed two groups of network structural positions, the centrality of individual adolescents in networks, and the homophily of their social ties regarding migration (same-migration). Multilevel logistic regression was used to model the association between network structural position and smoking, alcohol use, and cannabis use. Results Compared with non-migrant adolescents, adolescents with migration backgrounds had similar relationship patterns. But almost half their social ties were with same-migration-background adolescents; non-migrants had few social ties to migrants. For adolescents with a migration background, a higher proportion of social ties with non-migrants was associated with increased use of cannabis (OR = 1.07, p = 0.03) and alcohol (OR = 1.08, p &#60; 0.01), but not with increased smoking ( p = 0.60). Popular migrant adolescents were at less risk of smoking, alcohol use, and cannabis use than popular non-migrant adolescents. Conclusion Homophily of social ties by migration background is noticeable in European schools. The tendency of migrant adolescents to have same-migration social ties may isolate them from non-migrant adolescents, but also reduces their risky health behaviours, in particular cannabis and alcohol use.</blockquote>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(\"<blockquote>{}</blockquote>\".format(ti)))\n",
    "display(HTML(\"<blockquote>{}</blockquote>\".format(abs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:An unexpected error occurred while tokenizing input\n",
      "The following traceback may be corrupted or invalid\n",
      "The error message is: ('EOF in multi-line string', (1, 63))\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FreqDist' object has no attribute 'inc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-e0ff344488e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m   \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-e0ff344488e7>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0msystems\u001b[0m \u001b[0mare\u001b[0m \u001b[0mgiven\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mThese\u001b[0m \u001b[0mcriteria\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0malgorithms\u001b[0m \u001b[1;32mfor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0mconstructing\u001b[0m \u001b[0ma\u001b[0m \u001b[0mminimal\u001b[0m \u001b[0msupporting\u001b[0m \u001b[0mset\u001b[0m \u001b[0mof\u001b[0m \u001b[0msolutions\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msolving\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m all the considered types of systems and systems of mixed types.\"\"\", incl_scores=True)\n\u001b[0m\u001b[1;32m     89\u001b[0m   \u001b[1;32mprint\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-e0ff344488e7>\u001b[0m in \u001b[0;36mextract\u001b[0;34m(self, text, incl_scores)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mphrase_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_generate_candidate_keywords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mword_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_calculate_word_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrase_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     phrase_scores = self._calculate_phrase_scores(\n\u001b[1;32m     68\u001b[0m       phrase_list, word_scores)\n",
      "\u001b[0;32m<ipython-input-29-e0ff344488e7>\u001b[0m in \u001b[0;36m_calculate_word_scores\u001b[0;34m(self, phrase_list)\u001b[0m\n\u001b[1;32m     42\u001b[0m       \u001b[0mdegree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misNumeric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mphrase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mword_freq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mword_degree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# other words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_freq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FreqDist' object has no attribute 'inc'"
     ]
    }
   ],
   "source": [
    "# Adapted from: github.com/aneesha/RAKE/rake.py\n",
    "from __future__ import division\n",
    "import operator\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "def isPunct(word):\n",
    "  return len(word) == 1 and word in string.punctuation\n",
    "\n",
    "def isNumeric(word):\n",
    "  try:\n",
    "    float(word) if '.' in word else int(word)\n",
    "    return True\n",
    "  except ValueError:\n",
    "    return False\n",
    "\n",
    "class RakeKeywordExtractor:\n",
    "\n",
    "  def __init__(self):\n",
    "    self.stopwords = set(nltk.corpus.stopwords.words())\n",
    "    self.top_fraction = 1 # consider top third candidate keywords by score\n",
    "\n",
    "  def _generate_candidate_keywords(self, sentences):\n",
    "    phrase_list = []\n",
    "    for sentence in sentences:\n",
    "      words = map(lambda x: \"|\" if x in self.stopwords else x,\n",
    "        nltk.word_tokenize(sentence.lower()))\n",
    "      phrase = []\n",
    "      for word in words:\n",
    "        if word == \"|\" or isPunct(word):\n",
    "          if len(phrase) > 0:\n",
    "            phrase_list.append(phrase)\n",
    "            phrase = []\n",
    "        else:\n",
    "          phrase.append(word)\n",
    "    return phrase_list\n",
    "\n",
    "  def _calculate_word_scores(self, phrase_list):\n",
    "    word_freq = nltk.FreqDist()\n",
    "    word_degree = nltk.FreqDist()\n",
    "    for phrase in phrase_list:\n",
    "      degree = len(filter(lambda x: not isNumeric(x), phrase)) - 1\n",
    "      for word in phrase:\n",
    "        word_freq.inc(word)\n",
    "        word_degree.inc(word, degree) # other words\n",
    "    for word in word_freq.keys():\n",
    "      word_degree[word] = word_degree[word] + word_freq[word] # itself\n",
    "    # word score = deg(w) / freq(w)\n",
    "    word_scores = {}\n",
    "    for word in word_freq.keys():\n",
    "      word_scores[word] = word_degree[word] / word_freq[word]\n",
    "    return word_scores\n",
    "\n",
    "  def _calculate_phrase_scores(self, phrase_list, word_scores):\n",
    "    phrase_scores = {}\n",
    "    for phrase in phrase_list:\n",
    "      phrase_score = 0\n",
    "      for word in phrase:\n",
    "        phrase_score += word_scores[word]\n",
    "      phrase_scores[\" \".join(phrase)] = phrase_score\n",
    "    return phrase_scores\n",
    "    \n",
    "  def extract(self, text, incl_scores=False):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    phrase_list = self._generate_candidate_keywords(sentences)\n",
    "    word_scores = self._calculate_word_scores(phrase_list)\n",
    "    phrase_scores = self._calculate_phrase_scores(\n",
    "      phrase_list, word_scores)\n",
    "    sorted_phrase_scores = sorted(phrase_scores.iteritems(),\n",
    "      key=operator.itemgetter(1), reverse=True)\n",
    "    n_phrases = len(sorted_phrase_scores)\n",
    "    if incl_scores:\n",
    "      return sorted_phrase_scores[0:int(n_phrases/self.top_fraction)]\n",
    "    else:\n",
    "      return map(lambda x: x[0],\n",
    "        sorted_phrase_scores[0:int(n_phrases/self.top_fraction)])\n",
    "\n",
    "def test():\n",
    "  rake = RakeKeywordExtractor()\n",
    "  keywords = rake.extract(\"\"\"\n",
    "Compatibility of systems of linear constraints over the set of natural \n",
    "numbers. Criteria of compatibility of a system of linear Diophantine \n",
    "equations, strict inequations, and nonstrict inequations are considered. \n",
    "Upper bounds for components of a minimal set of solutions and algorithms \n",
    "of construction of minimal generating sets of solutions for all types of \n",
    "systems are given. These criteria and the corresponding algorithms for \n",
    "constructing a minimal supporting set of solutions can be used in solving \n",
    "all the considered types of systems and systems of mixed types.\"\"\", incl_scores=True)\n",
    "  print keywords\n",
    "  \n",
    "if __name__ == \"__main__\":\n",
    "  test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_candidate_chunks(text, grammar=r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}'):\n",
    "    import itertools, nltk, string\n",
    "    \n",
    "    # exclude candidates that are stop words or entirely punctuation\n",
    "    punct = set(string.punctuation)\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    # tokenize, POS-tag, and chunk using regular expressions\n",
    "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
    "    tagged_sents = nltk.pos_tag_sents(nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(text))\n",
    "    all_chunks = list(itertools.chain.from_iterable(nltk.chunk.tree2conlltags(chunker.parse(tagged_sent))\n",
    "                                                    for tagged_sent in tagged_sents))\n",
    "    # join constituent chunk words into a single chunked phrase\n",
    "    candidates = [' '.join(word for word, pos, chunk in group).lower()\n",
    "                  for key, group in itertools.groupby(all_chunks, lambda (word,pos,chunk): chunk != 'O') if key]\n",
    "\n",
    "    return [cand for cand in candidates\n",
    "            if cand not in stop_words and not all(char in punct for char in cand)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adolescent',\n",
       " 'adolescents',\n",
       " 'adolescents with migration backgrounds',\n",
       " 'aim',\n",
       " 'alcohol',\n",
       " 'alcohol use',\n",
       " 'association between network structural position',\n",
       " 'background social integration',\n",
       " 'cannabis use',\n",
       " 'cannabis use than popular non-migrant adolescents',\n",
       " 'centrality of individual adolescents',\n",
       " 'composition',\n",
       " 'composition of adolescents',\n",
       " 'conclusion homophily of social ties',\n",
       " 'european cities in belgium',\n",
       " 'european schools',\n",
       " 'few social ties',\n",
       " 'finland',\n",
       " 'first-generation migrants',\n",
       " 'friends',\n",
       " 'germany',\n",
       " 'groups of network structural positions',\n",
       " 'health behaviours',\n",
       " 'health of adolescents',\n",
       " 'homophily',\n",
       " 'italy',\n",
       " 'language at home',\n",
       " 'literature',\n",
       " 'little attention',\n",
       " 'major concern in multicultural societies',\n",
       " 'method',\n",
       " 'migrants',\n",
       " 'migration',\n",
       " 'migration background',\n",
       " 'migration status',\n",
       " 'multilevel logistic regression',\n",
       " 'netherlands',\n",
       " 'networks',\n",
       " 'non-migrant adolescents',\n",
       " 'non-migrants',\n",
       " 'p',\n",
       " 'p =',\n",
       " 'particular cannabis',\n",
       " 'popular migrant adolescents',\n",
       " 'portugal',\n",
       " 'proportion of social ties',\n",
       " 'results',\n",
       " 'risk of smoking',\n",
       " 'risky health behaviours',\n",
       " 'same-migration',\n",
       " 'same-migration social ties',\n",
       " 'same-migration-background adolescents',\n",
       " 'schools',\n",
       " 'second-generation migrants',\n",
       " 'secondary-school adolescents',\n",
       " 'silne study',\n",
       " 'similar relationship patterns',\n",
       " 'smoking',\n",
       " 'social network design',\n",
       " 'social networks',\n",
       " 'social ties',\n",
       " 'study',\n",
       " 'substance use',\n",
       " 'tendency of migrant adolescents',\n",
       " 'use of cannabis',\n",
       " 'wider determinants'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(extract_candidate_chunks(abs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_candidate_words(text, good_tags=set(['JJ','JJR','JJS','NN','NNP','NNS','NNPS'])):\n",
    "    import itertools, nltk, string\n",
    "\n",
    "    # exclude candidates that are stop words or entirely punctuation\n",
    "    punct = set(string.punctuation)\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    # tokenize and POS-tag words\n",
    "    tagged_words = itertools.chain.from_iterable(nltk.pos_tag_sents(nltk.word_tokenize(sent)\n",
    "                                                                    for sent in nltk.sent_tokenize(text)))\n",
    "    # filter on certain POS tags and lowercase all words\n",
    "    candidates = [word.lower() for word, tag in tagged_words\n",
    "                  if tag in good_tags and word.lower() not in stop_words\n",
    "                  and not all(char in punct for char in word)]\n",
    "\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adolescent',\n",
       " 'adolescents',\n",
       " 'aim',\n",
       " 'alcohol',\n",
       " 'association',\n",
       " 'attention',\n",
       " 'background',\n",
       " 'backgrounds',\n",
       " 'behaviours',\n",
       " 'belgium',\n",
       " 'best',\n",
       " 'cannabis',\n",
       " 'centrality',\n",
       " 'cities',\n",
       " 'closest',\n",
       " 'composition',\n",
       " 'concern',\n",
       " 'conclusion',\n",
       " 'design',\n",
       " 'determinants',\n",
       " 'european',\n",
       " 'finland',\n",
       " 'first-generation',\n",
       " 'friends',\n",
       " 'germany',\n",
       " 'groups',\n",
       " 'health',\n",
       " 'higher',\n",
       " 'home',\n",
       " 'homophily',\n",
       " 'individual',\n",
       " 'integration',\n",
       " 'italy',\n",
       " 'language',\n",
       " 'less',\n",
       " 'literature',\n",
       " 'little',\n",
       " 'logistic',\n",
       " 'major',\n",
       " 'method',\n",
       " 'migrant',\n",
       " 'migrants',\n",
       " 'migration',\n",
       " 'multicultural',\n",
       " 'multilevel',\n",
       " 'netherlands',\n",
       " 'network',\n",
       " 'networks',\n",
       " 'non-migrant',\n",
       " 'non-migrants',\n",
       " 'noticeable',\n",
       " 'p',\n",
       " 'particular',\n",
       " 'patterns',\n",
       " 'popular',\n",
       " 'portugal',\n",
       " 'position',\n",
       " 'positions',\n",
       " 'proportion',\n",
       " 'regression',\n",
       " 'relationship',\n",
       " 'results',\n",
       " 'risk',\n",
       " 'risky',\n",
       " 'same-migration',\n",
       " 'same-migration-background',\n",
       " 'schools',\n",
       " 'second-generation',\n",
       " 'secondary-school',\n",
       " 'silne',\n",
       " 'similar',\n",
       " 'smoking',\n",
       " 'social',\n",
       " 'societies',\n",
       " 'status',\n",
       " 'structural',\n",
       " 'study',\n",
       " 'substance',\n",
       " 'tendency',\n",
       " 'ties',\n",
       " 'use',\n",
       " 'wider'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(extract_candidate_words(abs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_keyphrases_by_tfidf(texts, candidates='chunks'):\n",
    "    import gensim, nltk\n",
    "    # extract candidates from each text in texts, either chunks or words\n",
    "    if candidates == 'chunks':\n",
    "        boc_texts = [extract_candidate_chunks(text) for text in texts]\n",
    "    elif candidates == 'words':\n",
    "        boc_texts = [extract_candidate_words(text) for text in texts]\n",
    "    # make gensim dictionary and corpus\n",
    "    dictionary = gensim.corpora.Dictionary(boc_texts)\n",
    "    corpus = [dictionary.doc2bow(boc_text) for boc_text in boc_texts]\n",
    "    # transform corpus with tf*idf model\n",
    "    tfidf = gensim.models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    \n",
    "    return corpus_tfidf, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<gensim.interfaces.TransformedCorpus at 0x11c119b0>,\n",
       " <gensim.corpora.dictionary.Dictionary at 0x9143128>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_keyphrases_by_tfidf(abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_keyphrases_by_textrank(text, n_keywords=0.05):\n",
    "    from itertools import takewhile, tee, izip\n",
    "    import networkx, nltk\n",
    "    \n",
    "    # tokenize for all words, and extract *candidate* words\n",
    "    words = [word.lower()\n",
    "             for sent in nltk.sent_tokenize(text)\n",
    "             for word in nltk.word_tokenize(sent)]\n",
    "    candidates = extract_candidate_words(text)\n",
    "    # build graph, each node is a unique candidate\n",
    "    graph = networkx.Graph()\n",
    "    graph.add_nodes_from(set(candidates))\n",
    "    # iterate over word-pairs, add unweighted edges into graph\n",
    "    def pairwise(iterable):\n",
    "        \"\"\"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\"\"\n",
    "        a, b = tee(iterable)\n",
    "        next(b, None)\n",
    "        return izip(a, b)\n",
    "    for w1, w2 in pairwise(candidates):\n",
    "        if w2:\n",
    "            graph.add_edge(*sorted([w1, w2]))\n",
    "    # score nodes using default pagerank algorithm, sort by score, keep top n_keywords\n",
    "    ranks = networkx.pagerank(graph)\n",
    "    if 0 < n_keywords < 1:\n",
    "        n_keywords = int(round(len(candidates) * n_keywords))\n",
    "    word_ranks = {word_rank[0]: word_rank[1]\n",
    "                  for word_rank in sorted(ranks.iteritems(), key=lambda x: x[1], reverse=True)[:n_keywords]}\n",
    "    keywords = set(word_ranks.keys())\n",
    "    # merge keywords into keyphrases\n",
    "    keyphrases = {}\n",
    "    j = 0\n",
    "    for i, word in enumerate(words):\n",
    "        if i < j:\n",
    "            continue\n",
    "        if word in keywords:\n",
    "            kp_words = list(takewhile(lambda x: x in keywords, words[i:i+10]))\n",
    "            avg_pagerank = sum(word_ranks[w] for w in kp_words) / float(len(kp_words))\n",
    "            keyphrases[' '.join(kp_words)] = avg_pagerank\n",
    "            # counter as hackish way to ensure merged keyphrases are non-overlapping\n",
    "            j = i + len(kp_words)\n",
    "    \n",
    "    return sorted(keyphrases.iteritems(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('adolescents', 0.057380093620592675),\n",
       " ('social', 0.04293214424986573),\n",
       " ('social network', 0.03181827802173994),\n",
       " ('social networks', 0.031355063212757585),\n",
       " ('social ties', 0.03123496123467995),\n",
       " ('migration', 0.027606613808646825),\n",
       " ('use', 0.023136127366676314),\n",
       " ('network', 0.02070441179361415),\n",
       " ('networks', 0.019777982175649442),\n",
       " ('migrants', 0.019295233310517638)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_keyphrases_by_textrank(abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_candidate_features(candidates, doc_text, doc_excerpt, doc_title):\n",
    "    import collections, math, nltk, re\n",
    "    \n",
    "    candidate_scores = collections.OrderedDict()\n",
    "    \n",
    "    # get word counts for document\n",
    "    doc_word_counts = collections.Counter(word.lower()\n",
    "                                          for sent in nltk.sent_tokenize(doc_text)\n",
    "                                          for word in nltk.word_tokenize(sent))\n",
    "    \n",
    "    for candidate in candidates:\n",
    "        \n",
    "        pattern = re.compile(r'\\b'+re.escape(candidate)+r'(\\b|[,;.!?]|\\s)', re.IGNORECASE)\n",
    "        \n",
    "        # frequency-based\n",
    "        # number of times candidate appears in document\n",
    "        cand_doc_count = len(pattern.findall(doc_text))\n",
    "        # count could be 0 for multiple reasons; shit happens in a simplified example\n",
    "        if not cand_doc_count:\n",
    "            print '**WARNING:', candidate, 'not found!'\n",
    "            continue\n",
    "    \n",
    "        # statistical\n",
    "        candidate_words = candidate.split()\n",
    "        max_word_length = max(len(w) for w in candidate_words)\n",
    "        term_length = len(candidate_words)\n",
    "        # get frequencies for term and constituent words\n",
    "        sum_doc_word_counts = float(sum(doc_word_counts[w] for w in candidate_words))\n",
    "        try:\n",
    "            # lexical cohesion doesn't make sense for 1-word terms\n",
    "            if term_length == 1:\n",
    "                lexical_cohesion = 0.0\n",
    "            else:\n",
    "                lexical_cohesion = term_length * (1 + math.log(cand_doc_count, 10)) * cand_doc_count / sum_doc_word_counts\n",
    "        except (ValueError, ZeroDivisionError) as e:\n",
    "            lexical_cohesion = 0.0\n",
    "        \n",
    "        # positional\n",
    "        # found in title, key excerpt\n",
    "        in_title = 1 if pattern.search(doc_title) else 0\n",
    "        in_excerpt = 1 if pattern.search(doc_excerpt) else 0\n",
    "        # first/last position, difference between them (spread)\n",
    "        doc_text_length = float(len(doc_text))\n",
    "        first_match = pattern.search(doc_text)\n",
    "        abs_first_occurrence = first_match.start() / doc_text_length\n",
    "        if cand_doc_count == 1:\n",
    "            spread = 0.0\n",
    "            abs_last_occurrence = abs_first_occurrence\n",
    "        else:\n",
    "            for last_match in pattern.finditer(doc_text):\n",
    "                pass\n",
    "            abs_last_occurrence = last_match.start() / doc_text_length\n",
    "            spread = abs_last_occurrence - abs_first_occurrence\n",
    "\n",
    "        candidate_scores[candidate] = {'term_count': cand_doc_count,\n",
    "                                       'term_length': term_length, 'max_word_length': max_word_length,\n",
    "                                       'spread': spread, 'lexical_cohesion': lexical_cohesion,\n",
    "                                       'in_excerpt': in_excerpt, 'in_title': in_title,\n",
    "                                       'abs_first_occurrence': abs_first_occurrence,\n",
    "                                       'abs_last_occurrence': abs_last_occurrence}\n",
    "\n",
    "    return candidate_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All this copied from: \"http://bdewilde.github.io/blog/2014/09/23/intro-to-automatic-keyphrase-extraction/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'candidate_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e0089c11d162>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcandidate_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"automatic keyphrase extraction\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'candidate_scores' is not defined"
     ]
    }
   ],
   "source": [
    "candidate_scores[\"automatic keyphrase extraction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A social network analysis of substance use among immigrant adolescents in six European cities.\n",
      "\n",
      "The tendency of migrant adolescents to have same-migration social ties may isolate them from non-migrant adolescents, but also reduces their risky health behaviours, in particular cannabis and alcohol use.\n",
      "\n",
      "Original Length 2251\n",
      "Summary Length 301\n",
      "Summary Ratio: 86.6281652599\n"
     ]
    }
   ],
   "source": [
    "# coding=UTF-8\n",
    "from __future__ import division\n",
    "import re\n",
    "\n",
    "# This is a naive text summarization algorithm\n",
    "# Created by Shlomi Babluki\n",
    "# April, 2013\n",
    "\n",
    "\n",
    "class SummaryTool(object):\n",
    "\n",
    "    # Naive method for splitting a text into sentences\n",
    "    def split_content_to_sentences(self, content):\n",
    "        content = content.replace(\"\\n\", \". \")\n",
    "        return content.split(\". \")\n",
    "\n",
    "    # Naive method for splitting a text into paragraphs\n",
    "    def split_content_to_paragraphs(self, content):\n",
    "        return content.split(\"\\n\\n\")\n",
    "\n",
    "    # Caculate the intersection between 2 sentences\n",
    "    def sentences_intersection(self, sent1, sent2):\n",
    "\n",
    "        # split the sentence into words/tokens\n",
    "        s1 = set(sent1.split(\" \"))\n",
    "        s2 = set(sent2.split(\" \"))\n",
    "\n",
    "        # If there is not intersection, just return 0\n",
    "        if (len(s1) + len(s2)) == 0:\n",
    "            return 0\n",
    "\n",
    "        # We normalize the result by the average number of words\n",
    "        return len(s1.intersection(s2)) / ((len(s1) + len(s2)) / 2)\n",
    "\n",
    "    # Format a sentence - remove all non-alphbetic chars from the sentence\n",
    "    # We'll use the formatted sentence as a key in our sentences dictionary\n",
    "    def format_sentence(self, sentence):\n",
    "        sentence = re.sub(r'\\W+', '', sentence)\n",
    "        return sentence\n",
    "\n",
    "    # Convert the content into a dictionary <K, V>\n",
    "    # k = The formatted sentence\n",
    "    # V = The rank of the sentence\n",
    "    def get_senteces_ranks(self, content):\n",
    "\n",
    "        # Split the content into sentences\n",
    "        sentences = self.split_content_to_sentences(content)\n",
    "\n",
    "        # Calculate the intersection of every two sentences\n",
    "        n = len(sentences)\n",
    "        values = [[0 for x in xrange(n)] for x in xrange(n)]\n",
    "        for i in range(0, n):\n",
    "            for j in range(0, n):\n",
    "                values[i][j] = self.sentences_intersection(sentences[i], sentences[j])\n",
    "\n",
    "        # Build the sentences dictionary\n",
    "        # The score of a sentences is the sum of all its intersection\n",
    "        sentences_dic = {}\n",
    "        for i in range(0, n):\n",
    "            score = 0\n",
    "            for j in range(0, n):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                score += values[i][j]\n",
    "            sentences_dic[self.format_sentence(sentences[i])] = score\n",
    "        return sentences_dic\n",
    "\n",
    "    # Return the best sentence in a paragraph\n",
    "    def get_best_sentence(self, paragraph, sentences_dic):\n",
    "\n",
    "        # Split the paragraph into sentences\n",
    "        sentences = self.split_content_to_sentences(paragraph)\n",
    "\n",
    "        # Ignore short paragraphs\n",
    "        if len(sentences) < 2:\n",
    "            return \"\"\n",
    "\n",
    "        # Get the best sentence according to the sentences dictionary\n",
    "        best_sentence = \"\"\n",
    "        max_value = 0\n",
    "        for s in sentences:\n",
    "            strip_s = self.format_sentence(s)\n",
    "            if strip_s:\n",
    "                if sentences_dic[strip_s] > max_value:\n",
    "                    max_value = sentences_dic[strip_s]\n",
    "                    best_sentence = s\n",
    "\n",
    "        return best_sentence\n",
    "\n",
    "    # Build the summary\n",
    "    def get_summary(self, title, content, sentences_dic):\n",
    "\n",
    "        # Split the content into paragraphs\n",
    "        paragraphs = self.split_content_to_paragraphs(content)\n",
    "\n",
    "        # Add the title\n",
    "        summary = []\n",
    "        summary.append(title.strip())\n",
    "        summary.append(\"\")\n",
    "\n",
    "        # Add the best sentence from each paragraph\n",
    "        for p in paragraphs:\n",
    "            sentence = self.get_best_sentence(p, sentences_dic).strip()\n",
    "            if sentence:\n",
    "                summary.append(sentence)\n",
    "\n",
    "        return (\"\\n\").join(summary)\n",
    "\n",
    "\n",
    "# Main method, just run \"python summary_tool.py\"\n",
    "def main():\n",
    "\n",
    "    # Demo\n",
    "    # Content from: \"http://thenextweb.com/apps/2013/03/21/swayy-discover-curate-content/\"\n",
    "\n",
    "    title = ti\n",
    "\n",
    "    content = abs\n",
    "\n",
    "    # Create a SummaryTool object\n",
    "    st = SummaryTool()\n",
    "\n",
    "    # Build the sentences dictionary\n",
    "    sentences_dic = st.get_senteces_ranks(content)\n",
    "\n",
    "    # Build the summary with the sentences dictionary\n",
    "    summary = st.get_summary(title, content, sentences_dic)\n",
    "\n",
    "    # Print the summary\n",
    "    print summary\n",
    "\n",
    "    # Print the ratio between the summary length and the original length\n",
    "    print \"\"\n",
    "    print \"Original Length %s\" % (len(title) + len(content))\n",
    "    print \"Summary Length %s\" % len(summary)\n",
    "    print \"Summary Ratio: %s\" % (100 - (100 * (len(summary) / (len(title) + len(content)))))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = \"A social network analysis of substance use among immigrant adolescents in six European cities. The tendency of migrant adolescents to have same-migration social ties may isolate them from non-migrant adolescents, but also reduces their risky health behaviours, in particular cannabis and alcohol use.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('adolescents', 0.11612227866212324)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(score_keyphrases_by_textrank(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sentence is about: social network analysis, substance use, immigrant adolescents, European cities, migrant adolescents, social ties, non-migrant adolescents, risky health behaviours, particular cannabis, alcohol use\n"
     ]
    }
   ],
   "source": [
    "# coding=UTF-8\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# This is a fast and simple noun phrase extractor (based on NLTK)\n",
    "# Feel free to use it, just keep a link back to this post\n",
    "# http://thetokenizer.com/2013/05/09/efficient-way-to-extract-the-main-topics-of-a-sentence/\n",
    "# Create by Shlomi Babluki\n",
    "# May, 2013\n",
    "\n",
    "\n",
    "# This is our fast Part of Speech tagger\n",
    "#############################################################################\n",
    "brown_train = brown.tagged_sents(categories='news')\n",
    "regexp_tagger = nltk.RegexpTagger(\n",
    "    [(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),\n",
    "     (r'(-|:|;)$', ':'),\n",
    "     (r'\\'*$', 'MD'),\n",
    "     (r'(The|the|A|a|An|an)$', 'AT'),\n",
    "     (r'.*able$', 'JJ'),\n",
    "     (r'^[A-Z].*$', 'NNP'),\n",
    "     (r'.*ness$', 'NN'),\n",
    "     (r'.*ly$', 'RB'),\n",
    "     (r'.*s$', 'NNS'),\n",
    "     (r'.*ing$', 'VBG'),\n",
    "     (r'.*ed$', 'VBD'),\n",
    "     (r'.*', 'NN')\n",
    "])\n",
    "unigram_tagger = nltk.UnigramTagger(brown_train, backoff=regexp_tagger)\n",
    "bigram_tagger = nltk.BigramTagger(brown_train, backoff=unigram_tagger)\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "# This is our semi-CFG; Extend it according to your own needs\n",
    "#############################################################################\n",
    "cfg = {}\n",
    "cfg[\"NNP+NNP\"] = \"NNP\"\n",
    "cfg[\"NN+NN\"] = \"NNI\"\n",
    "cfg[\"NNI+NN\"] = \"NNI\"\n",
    "cfg[\"JJ+JJ\"] = \"JJ\"\n",
    "cfg[\"JJ+NN\"] = \"NNI\"\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "class NPExtractor(object):\n",
    "\n",
    "    def __init__(self, sentence):\n",
    "        self.sentence = sentence\n",
    "\n",
    "    # Split the sentence into singlw words/tokens\n",
    "    def tokenize_sentence(self, sentence):\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        return tokens\n",
    "\n",
    "    # Normalize brown corpus' tags (\"NN\", \"NN-PL\", \"NNS\" > \"NN\")\n",
    "    def normalize_tags(self, tagged):\n",
    "        n_tagged = []\n",
    "        for t in tagged:\n",
    "            if t[1] == \"NP-TL\" or t[1] == \"NP\":\n",
    "                n_tagged.append((t[0], \"NNP\"))\n",
    "                continue\n",
    "            if t[1].endswith(\"-TL\"):\n",
    "                n_tagged.append((t[0], t[1][:-3]))\n",
    "                continue\n",
    "            if t[1].endswith(\"S\"):\n",
    "                n_tagged.append((t[0], t[1][:-1]))\n",
    "                continue\n",
    "            n_tagged.append((t[0], t[1]))\n",
    "        return n_tagged\n",
    "\n",
    "    # Extract the main topics from the sentence\n",
    "    def extract(self):\n",
    "\n",
    "        tokens = self.tokenize_sentence(self.sentence)\n",
    "        tags = self.normalize_tags(bigram_tagger.tag(tokens))\n",
    "\n",
    "        merge = True\n",
    "        while merge:\n",
    "            merge = False\n",
    "            for x in range(0, len(tags) - 1):\n",
    "                t1 = tags[x]\n",
    "                t2 = tags[x + 1]\n",
    "                key = \"%s+%s\" % (t1[1], t2[1])\n",
    "                value = cfg.get(key, '')\n",
    "                if value:\n",
    "                    merge = True\n",
    "                    tags.pop(x)\n",
    "                    tags.pop(x)\n",
    "                    match = \"%s %s\" % (t1[0], t2[0])\n",
    "                    pos = value\n",
    "                    tags.insert(x, (match, pos))\n",
    "                    break\n",
    "\n",
    "        matches = []\n",
    "        for t in tags:\n",
    "            if t[1] == \"NNP\" or t[1] == \"NNI\":\n",
    "            #if t[1] == \"NNP\" or t[1] == \"NNI\" or t[1] == \"NN\":\n",
    "                matches.append(t[0])\n",
    "        return matches\n",
    "\n",
    "\n",
    "# Main method, just run \"python np_extractor.py\"\n",
    "def main():\n",
    "\n",
    "    sentence = text\n",
    "    np_extractor = NPExtractor(sentence)\n",
    "    result = np_extractor.extract()\n",
    "    print \"This sentence is about: %s\" % \", \".join(result)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Excellent notes [here](\"https://thetokenizer.com/2013/04/28/build-your-own-summary-tool/\")\n",
    "\n",
    "\"Essentially, the algorithm takes the sentences in every paragraph of text, and gives them a score based on how many words in the sentence intersect(also occur) in other sentences of the text. The highest ranked sentences from each paragraph are chosen and put in order.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% Completed\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-d0442957416d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-d0442957416d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[1;34m\"%s%% Completed\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mcount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[1;32mif\u001b[0m \u001b[0mner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_person\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0mpeople\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"100% Done!\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-d0442957416d>\u001b[0m in \u001b[0;36mis_person\u001b[0;34m(self, possible_name)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"key\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfreebase_server\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"result\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0moption\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "# coding=UTF-8\n",
    "from __future__ import division\n",
    "import nltk\n",
    "import re\n",
    "import requests\n",
    "\n",
    "# Add your freebase key here\n",
    "# If you don't have one, register at https://code.google.com/apis/console\n",
    "FREEBASE_KEY = \"AIzaSyBo8z56-cVJVt_HQT2I24Ob7YpVqY5ouNo\"\n",
    "\n",
    "pattern = \"(?P<name>(([A-Z]+)([a-z]*)(\\s)?)*)\"\n",
    "\n",
    "\n",
    "class NER(object):\n",
    "\n",
    "    def __init__(self, text, key=None):\n",
    "        self.text = text\n",
    "        self.sentences = self.split_text(text)\n",
    "        self.results = []\n",
    "        self.key = key\n",
    "\n",
    "    def split_text(self, text):\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        stripped_sentences = []\n",
    "        for sentence in sentences:\n",
    "            stripped_sentences.append(sentence.strip())\n",
    "        return stripped_sentences\n",
    "\n",
    "    def get_options(self):\n",
    "        options = set()\n",
    "        for s in self.sentences:\n",
    "            f = re.finditer(pattern, s)\n",
    "            for a in f:\n",
    "                o = a.group(\"name\").strip()\n",
    "                parts = o.split(\" \")\n",
    "                if len(parts) > 1:\n",
    "                    options.add(o)\n",
    "                    if len(parts) > 2:\n",
    "                        extra_options = nltk.ngrams(parts, 2)\n",
    "                        for e in extra_options:\n",
    "                            options.add(\" \".join(e))\n",
    "\n",
    "        return options\n",
    "\n",
    "    def is_person(self, possible_name):\n",
    "        # Run first with filter\n",
    "        freebase_server = \"https://www.googleapis.com/freebase/v1/search\"\n",
    "        params = {\n",
    "            \"query\": possible_name,\n",
    "            \"filter\": \"(any type:/people/person)\"\n",
    "        }\n",
    "        if self.key:\n",
    "            params[\"key\"] = self.key\n",
    "        options = requests.get(freebase_server, params=params).json.get(\"result\", \"\")\n",
    "        if options:\n",
    "            for option in options:\n",
    "                if possible_name == option[\"name\"]:\n",
    "                    # Run without filter and validate\n",
    "                    mid = option[\"mid\"]\n",
    "                    params = {\"query\": possible_name}\n",
    "                    if self.key:\n",
    "                        params[\"key\"] = self.key\n",
    "                    compare = requests.get(freebase_server, params=params).json.get(\"result\", \"\")\n",
    "                    for result in compare:\n",
    "                        if result[\"mid\"] == mid:\n",
    "                            return True\n",
    "        return False\n",
    "\n",
    "# Main method, just run \"python freebase_ner.py\"\n",
    "def main():\n",
    "\n",
    "    # I took this article from:\n",
    "    # http://keepingscore.blogs.time.com/2013/08/21/with-beanball-justice-baseball-makes-a-rod-sympathetic/?iid=sp-main-lead\n",
    "\n",
    "    text = \"\"\"\n",
    "    If the Alex Rodriguez saga has taught sports fans anything — besides the fact that the guy has an innate ability to tune out reality and still occasionally hit a baseball very far — it’s this: when sports leagues play judge and jury, things can get mighty awkward.\n",
    "    On Sunday night, Boston’s Ryan Dempster plunked Rodriguez in the ribs with a 92-m.p.h. (148 km/h) fastball, after brushing him back on one pitch, and working him inside on two others. Dempster’s intent was clear: he was drilling Rodriguez on purpose. The umps did not eject Dempster. Yankees manager Joe Girardi then went ballistic, and got tossed himself. But baseball had no choice but to discipline Dempster. On Tuesday, MLB revealed its verdict: five games and an undisclosed fine for Dempster (Girardi was also fined).\n",
    "    And thus, baseball suspended one player for hitting another player that baseball has already suspended, and doesn’t want on the field.\n",
    "    Baseball’s decision, on the surface, may appear noble. MLB might not like A-Rod. But a fine and suspension shows that pitchers don’t have carte blanche to peg him. Still, I don’t believe baseball went far enough. First off, beanball justice is silly and dangerous. A-Rod deserves any and all loathing. But no one deserves to be thrown out. Baseballs hurt. And beanballs are cheap shots, since batters rarely have time to dodge them. Yes, A-Rod was unharmed. But he could have gotten injured. If Dempster wanted to express his disgust at A-Rod, he could have jawed at him from the pitcher’s mound. Or picked a fight, rather than hiding behind a 92-m.p.h. dart to the ribs.\n",
    "    Plus, if Dempster does not appeal, he doesn’t have to miss a start during his five-game break, since Boston has off days on Thursday and Monday. As the Providence Journal notes, if Jon Lester and Jake Peavy pitch on regular rest this weekend, Dempster can return in time for his next appearance. Plus, Dempster is still getting paid. If it weren’t for the fine — I’m guessing it won’t drain Dempster’s wallet, as he’s making $13.25 million this season — the suspension would be a vacation.\n",
    "    Dempster deserves to miss a start. Would baseball have issued a tougher punishment, had A-Rod not been so contemptible? Baseball can claim equal justice for all. When baseball is in charge of both policing the game, and handing down the penalties it sees fit, fans can never know for sure.\n",
    "    \"\"\"\n",
    "\n",
    "    text = text.decode(\"utf-8\")\n",
    "    ner = NER(text, FREEBASE_KEY)\n",
    "    options = ner.get_options()\n",
    "    people = []\n",
    "    count = 0\n",
    "    for option in options:\n",
    "        print \"%s%% Completed\" % (count / len(options) * 100)\n",
    "        count += 1\n",
    "        if ner.is_person(option):\n",
    "            people.append(option)\n",
    "    print \"100% Done!\\n\"\n",
    "    print \", \".join(people)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "exampleArray = [u'Keywords, which we define as a sequence of one or more words, provide a compact representation of a document’s content.']\n",
    "type(exampleArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Keywords/NNS\n",
      "  ,/,\n",
      "  which/WDT\n",
      "  we/PRP\n",
      "  define/VBP\n",
      "  as/IN\n",
      "  a/DT\n",
      "  sequence/NN\n",
      "  of/IN\n",
      "  one/CD\n",
      "  or/CC\n",
      "  more/JJR\n",
      "  words/NNS\n",
      "  ,/,\n",
      "  provide/VB\n",
      "  a/DT\n",
      "  compact/JJ\n",
      "  representation/NN\n",
      "  of/IN\n",
      "  a/DT\n",
      "  document's/NN\n",
      "  content/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "def processContent():\n",
    "    try:\n",
    "        for item in exampleArray:\n",
    "            tokenized = nltk.word_tokenize(item)\n",
    "            tagged = nltk.pos_tag(tokenized)\n",
    "\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            print(chunked)\n",
    "            #chunked.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "    \n",
    "processContent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Compatibility', 'NN'), ('of', 'IN'), ('systems', 'NNS'), ('of', 'IN'), ('linear', 'JJ'), ('constraints', 'NNS'), ('over', 'IN'), ('the', 'DT'), ('set', 'NN'), ('of', 'IN'), ('natural', 'JJ'), ('numbers', 'NNS'), ('.', '.'), ('Criteria', 'NNP'), ('of', 'IN'), ('compatibility', 'NN')]\n",
      "[('of', 'IN'), ('a', 'DT'), ('system', 'NN'), ('of', 'IN'), ('linear', 'JJ'), ('Diophantine', 'NNP'), ('equations', 'NNS'), (',', ','), ('strict', 'JJ'), ('inequations', 'NNS'), (',', ','), ('and', 'CC'), ('nonstrict', 'JJ'), ('inequations', 'NNS'), ('are', 'VBP'), ('considered', 'VBN'), ('.', '.')]\n",
      "[('Upper', 'NNP'), ('bounds', 'VBZ'), ('for', 'IN'), ('components', 'NNS'), ('of', 'IN'), ('a', 'DT'), ('minimal', 'JJ'), ('set', 'NN'), ('of', 'IN'), ('solutions', 'NNS'), ('and', 'CC'), ('algorithms', 'NN'), ('of', 'IN'), ('construction', 'NN'), ('of', 'IN'), ('minimal', 'JJ'), ('generating', 'VBG')]\n",
      "[('sets', 'NNS'), ('of', 'IN'), ('solutions', 'NNS'), ('for', 'IN'), ('all', 'DT'), ('types', 'NNS'), ('of', 'IN'), ('systems', 'NNS'), ('are', 'VBP'), ('given', 'VBN'), ('.', '.'), ('These', 'DT'), ('criteria', 'NNS'), ('and', 'CC'), ('the', 'DT'), ('corresponding', 'JJ'), ('algorithms', 'NN')]\n",
      "[('for', 'IN'), ('constructing', 'VBG'), ('a', 'DT'), ('minimal', 'JJ'), ('supporting', 'NN'), ('set', 'NN'), ('of', 'IN'), ('solutions', 'NNS'), ('can', 'MD'), ('be', 'VB'), ('used', 'VBN'), ('in', 'IN'), ('solving', 'VBG'), ('all', 'PDT'), ('the', 'DT'), ('considered', 'VBN'), ('types', 'NNS'), ('of', 'IN')]\n",
      "[('systems', 'NNS'), ('and', 'CC'), ('systems', 'NNS'), ('of', 'IN'), ('mixed', 'JJ'), ('types', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "contentArray = [\"Compatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility \",\n",
    "       \"of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. \",\n",
    "       \"Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating\",\n",
    "       \" sets of solutions for all types of systems are given. These criteria and the corresponding algorithms \",\n",
    "       \"for constructing a minimal supporting set of solutions can be used in solving all the considered types of \",\n",
    "       \"systems and systems of mixed types.\"]\n",
    "\n",
    "def processLanguage():\n",
    "    try:\n",
    "        for item in contentArray:\n",
    "            tokenized = nltk.word_tokenize(item)\n",
    "            tagged = nltk.pos_tag(tokenized)\n",
    "            print tagged\n",
    "\n",
    "            namedEnt = nltk.ne_chunk(tagged)\n",
    "            #namedEnt.draw()\n",
    "\n",
    "            #time.sleep(1)\n",
    "\n",
    "    except Exception, e:\n",
    "        print str(e)\n",
    "        \n",
    "processLanguage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def processor(data):\n",
    "    namedEntArray = []\n",
    "    try:\n",
    "        tokenized = nltk.word_tokenize(data)\n",
    "        tagged = nltk.pos_tag(tokenized)\n",
    "        namedEnt = nltk.ne_chunk(tagged, binary=True)\n",
    "\n",
    "        entities = re.findall(r'NE\\s(.*?)/',str(namedEnt))\n",
    "        #('not', 'RB')\n",
    "        descriptives = re.findall(r'\\(\\'(\\w*)\\',\\s\\'JJ\\w?\\'', str(tagged))\n",
    "        if len(entities) > 1:\n",
    "            pass\n",
    "        elif len(entities) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            print '_________________________'\n",
    "            print 'Named:',entities[0]\n",
    "            print 'Descriptions:'\n",
    "            for eachDesc in descriptives:\n",
    "                print eachDesc\n",
    "\n",
    "    except Exception, e:\n",
    "        print 'failed in the main try of processor'\n",
    "        print str(e)\n",
    "        #time.sleep(555)               \n",
    "                \n",
    "                \n",
    "processor(contentArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taylor Swift - Singer-songwriter (879.619446)\n",
      "\n",
      "Taylor Alison Swift is an American singer-songwriter. One of the most popular contemporary female recording artists, she is known for narrative songs about her personal life, which has received much media attention.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Example of Python client calling Knowledge Graph Search API.\"\"\"\n",
    "import json\n",
    "import urllib\n",
    "\n",
    "api_key = open('api_key.txt').read()\n",
    "query = 'Taylor Swift'\n",
    "service_url = 'https://kgsearch.googleapis.com/v1/entities:search'\n",
    "params = {\n",
    "    'query': query,\n",
    "    'limit': 1,\n",
    "    'indent': True,\n",
    "    'key': api_key,\n",
    "}\n",
    "url = service_url + '?' + urllib.urlencode(params)\n",
    "response = json.loads(urllib.urlopen(url).read())\n",
    "for element in response['itemListElement']:\n",
    "  print element['result']['name'] + ' - ' + element['result']['description'] + ' (' + str(element['resultScore']) + ')'\n",
    "  print \"\"\n",
    "  print element['result']['detailedDescription']['articleBody']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'@context': {u'goog': u'http://schema.googleapis.com/', u'resultScore': u'goog:resultScore', u'@vocab': u'http://schema.org/', u'kg': u'http://g.co/kg', u'EntitySearchResult': u'goog:EntitySearchResult', u'detailedDescription': u'goog:detailedDescription'}, u'itemListElement': [{u'resultScore': 879.619446, u'@type': u'EntitySearchResult', u'result': {u'description': u'Singer-songwriter', u'url': u'http://www.taylorswift.com/', u'image': {u'contentUrl': u'http://t1.gstatic.com/images?q=tbn:ANd9GcTrQOavDXORvN_YAFZakAlE2O0yzXGAX4jVN3q3POqoNlCwOvdY', u'url': u'https://en.wikipedia.org/wiki/Taylor_Swift_discography', u'license': u'http://creativecommons.org/licenses/by-sa/2.0'}, u'detailedDescription': {u'url': u'https://en.wikipedia.org/wiki/Taylor_Swift', u'articleBody': u'Taylor Alison Swift is an American singer-songwriter. One of the most popular contemporary female recording artists, she is known for narrative songs about her personal life, which has received much media attention.\\n', u'license': u'https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License'}, u'@id': u'kg:/m/0dl567', u'@type': [u'Thing', u'Person'], u'name': u'Taylor Swift'}}], u'@type': u'ItemList'}\n"
     ]
    }
   ],
   "source": [
    "print response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'resultScore': 879.619446, u'@type': u'EntitySearchResult', u'result': {u'description': u'Singer-songwriter', u'url': u'http://www.taylorswift.com/', u'image': {u'contentUrl': u'http://t1.gstatic.com/images?q=tbn:ANd9GcTrQOavDXORvN_YAFZakAlE2O0yzXGAX4jVN3q3POqoNlCwOvdY', u'url': u'https://en.wikipedia.org/wiki/Taylor_Swift_discography', u'license': u'http://creativecommons.org/licenses/by-sa/2.0'}, u'detailedDescription': {u'url': u'https://en.wikipedia.org/wiki/Taylor_Swift', u'articleBody': u'Taylor Alison Swift is an American singer-songwriter. One of the most popular contemporary female recording artists, she is known for narrative songs about her personal life, which has received much media attention.\\n', u'license': u'https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License'}, u'@id': u'kg:/m/0dl567', u'@type': [u'Thing', u'Person'], u'name': u'Taylor Swift'}}]\n"
     ]
    }
   ],
   "source": [
    "print response['itemListElement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'@context': {u'@vocab': u'http://schema.org/',\n",
      "               u'EntitySearchResult': u'goog:EntitySearchResult',\n",
      "               u'detailedDescription': u'goog:detailedDescription',\n",
      "               u'goog': u'http://schema.googleapis.com/',\n",
      "               u'kg': u'http://g.co/kg',\n",
      "               u'resultScore': u'goog:resultScore'},\n",
      " u'@type': u'ItemList',\n",
      " u'itemListElement': [{u'@type': u'EntitySearchResult',\n",
      "                       u'result': {u'@id': u'kg:/m/0dl567',\n",
      "                                   u'@type': [u'Thing', u'Person'],\n",
      "                                   u'description': u'Singer-songwriter',\n",
      "                                   u'detailedDescription': {u'articleBody': u'Taylor Alison Swift is an American singer-songwriter. One of the most popular contemporary female recording artists, she is known for narrative songs about her personal life, which has received much media attention.\\n',\n",
      "                                                            u'license': u'https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License',\n",
      "                                                            u'url': u'https://en.wikipedia.org/wiki/Taylor_Swift'},\n",
      "                                   u'image': {u'contentUrl': u'http://t1.gstatic.com/images?q=tbn:ANd9GcTrQOavDXORvN_YAFZakAlE2O0yzXGAX4jVN3q3POqoNlCwOvdY',\n",
      "                                              u'license': u'http://creativecommons.org/licenses/by-sa/2.0',\n",
      "                                              u'url': u'https://en.wikipedia.org/wiki/Taylor_Swift_discography'},\n",
      "                                   u'name': u'Taylor Swift',\n",
      "                                   u'url': u'http://www.taylorswift.com/'},\n",
      "                       u'resultScore': 879.619446}]}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'@type': u'EntitySearchResult',\n",
      "  u'result': {u'@id': u'kg:/m/0dl567',\n",
      "              u'@type': [u'Thing', u'Person'],\n",
      "              u'description': u'Singer-songwriter',\n",
      "              u'detailedDescription': {u'articleBody': u'Taylor Alison Swift is an American singer-songwriter. One of the most popular contemporary female recording artists, she is known for narrative songs about her personal life, which has received much media attention.\\n',\n",
      "                                       u'license': u'https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License',\n",
      "                                       u'url': u'https://en.wikipedia.org/wiki/Taylor_Swift'},\n",
      "              u'image': {u'contentUrl': u'http://t1.gstatic.com/images?q=tbn:ANd9GcTrQOavDXORvN_YAFZakAlE2O0yzXGAX4jVN3q3POqoNlCwOvdY',\n",
      "                         u'license': u'http://creativecommons.org/licenses/by-sa/2.0',\n",
      "                         u'url': u'https://en.wikipedia.org/wiki/Taylor_Swift_discography'},\n",
      "              u'name': u'Taylor Swift',\n",
      "              u'url': u'http://www.taylorswift.com/'},\n",
      "  u'resultScore': 879.619446}]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(response['itemListElement'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taylor Swift\n"
     ]
    }
   ],
   "source": [
    "for element in response['itemListElement']:\n",
    "  print element['result']['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding=UTF-8\n",
    "from __future__ import division\n",
    "import nltk\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import urllib\n",
    "\n",
    "pattern = \"(?P<name>(([A-Z]+)([a-z]*)(\\s)?)*)\"\n",
    "\n",
    "class NER(object):\n",
    "\n",
    "    def __init__(self, text, key=None):\n",
    "        self.text = text\n",
    "        self.sentences = self.split_text(text)\n",
    "        self.results = []\n",
    "        self.key = key\n",
    "\n",
    "    def split_text(self, text):\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        stripped_sentences = []\n",
    "        for sentence in sentences:\n",
    "            stripped_sentences.append(sentence.strip())\n",
    "        return stripped_sentences\n",
    "\n",
    "    def get_options(self):\n",
    "        options = set()\n",
    "        for s in self.sentences:\n",
    "            f = re.finditer(pattern, s)\n",
    "            for a in f:\n",
    "                o = a.group(\"name\").strip()\n",
    "                parts = o.split(\" \")\n",
    "                if len(parts) > 1:\n",
    "                    options.add(o)\n",
    "                    if len(parts) > 2:\n",
    "                        extra_options = nltk.ngrams(parts, 2)\n",
    "                        for e in extra_options:\n",
    "                            options.add(\" \".join(e))\n",
    "\n",
    "        return options\n",
    "\n",
    "\n",
    "    def is_person(self, possible_name):\n",
    "        # Run first with filter\n",
    "        api_key = open('api_key.txt').read()\n",
    "        service_url = 'https://kgsearch.googleapis.com/v1/entities:search'\n",
    "        params = {\n",
    "            'query': possible_name,\n",
    "            'limit': 10,\n",
    "            'indent': True,\n",
    "            'key': api_key,\n",
    "        }\n",
    "        if self.key:\n",
    "            params[\"key\"] = self.key\n",
    "        url = service_url + '?' + urllib.urlencode(params)\n",
    "        response = json.loads(urllib.urlopen(url).read())\n",
    "        options = response['itemListElement']['name']\n",
    "        if options:\n",
    "            for option in options:\n",
    "                if possible_name == option[\"name\"]:\n",
    "                    # Run without filter and validate\n",
    "                    mid = option[\"mid\"]\n",
    "                    params = {\"query\": possible_name}\n",
    "                    if self.key:\n",
    "                        params[\"key\"] = self.key\n",
    "                    compare = requests.get(freebase_server, params=params).json.get(\"result\", \"\")\n",
    "                    for result in compare:\n",
    "                        if result[\"mid\"] == mid:\n",
    "                            return True\n",
    "        return False\n",
    "\n",
    "for element in response['itemListElement']:\n",
    "  print element['result']['name'] + ' (' + str(element['resultScore']) + ')'\n",
    "\n",
    "# returns eg:\n",
    "# Taylor Swift (879.619446)\n",
    "# Taylor Swift (531.308594)\n",
    "# 2009 MTV Video Music Awards (341.488831)\n",
    "\n",
    "\n",
    "# Main method, just run \"python freebase_ner.py\"\n",
    "def main():\n",
    "\n",
    "    # I took this article from:\n",
    "    # http://keepingscore.blogs.time.com/2013/08/21/with-beanball-justice-baseball-makes-a-rod-sympathetic/?iid=sp-main-lead\n",
    "\n",
    "    text = \"\"\"\n",
    "    If the Alex Rodriguez saga has taught sports fans anything — besides the fact that the guy has an innate ability to tune out reality and still occasionally hit a baseball very far — it’s this: when sports leagues play judge and jury, things can get mighty awkward.\n",
    "    On Sunday night, Boston’s Ryan Dempster plunked Rodriguez in the ribs with a 92-m.p.h. (148 km/h) fastball, after brushing him back on one pitch, and working him inside on two others. Dempster’s intent was clear: he was drilling Rodriguez on purpose. The umps did not eject Dempster. Yankees manager Joe Girardi then went ballistic, and got tossed himself. But baseball had no choice but to discipline Dempster. On Tuesday, MLB revealed its verdict: five games and an undisclosed fine for Dempster (Girardi was also fined).\n",
    "    And thus, baseball suspended one player for hitting another player that baseball has already suspended, and doesn’t want on the field.\n",
    "    Baseball’s decision, on the surface, may appear noble. MLB might not like A-Rod. But a fine and suspension shows that pitchers don’t have carte blanche to peg him. Still, I don’t believe baseball went far enough. First off, beanball justice is silly and dangerous. A-Rod deserves any and all loathing. But no one deserves to be thrown out. Baseballs hurt. And beanballs are cheap shots, since batters rarely have time to dodge them. Yes, A-Rod was unharmed. But he could have gotten injured. If Dempster wanted to express his disgust at A-Rod, he could have jawed at him from the pitcher’s mound. Or picked a fight, rather than hiding behind a 92-m.p.h. dart to the ribs.\n",
    "    Plus, if Dempster does not appeal, he doesn’t have to miss a start during his five-game break, since Boston has off days on Thursday and Monday. As the Providence Journal notes, if Jon Lester and Jake Peavy pitch on regular rest this weekend, Dempster can return in time for his next appearance. Plus, Dempster is still getting paid. If it weren’t for the fine — I’m guessing it won’t drain Dempster’s wallet, as he’s making $13.25 million this season — the suspension would be a vacation.\n",
    "    Dempster deserves to miss a start. Would baseball have issued a tougher punishment, had A-Rod not been so contemptible? Baseball can claim equal justice for all. When baseball is in charge of both policing the game, and handing down the penalties it sees fit, fans can never know for sure.\n",
    "    \"\"\"\n",
    "\n",
    "    text = text.decode(\"utf-8\")\n",
    "    ner = NER(text, FREEBASE_KEY)\n",
    "    options = ner.get_options()\n",
    "    people = []\n",
    "    count = 0\n",
    "    for option in options:\n",
    "        print \"%s%% Completed\" % (count / len(options) * 100)\n",
    "        count += 1\n",
    "        if ner.is_person(option):\n",
    "            people.append(option)\n",
    "    print \"100% Done!\\n\"\n",
    "    print \", \".join(people)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
