{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification 2\n",
    "\n",
    "From [\"Classifying Text Using Machine Learning\"](https://app.pluralsight.com/player?course=python-natural-language-processing&author=swetha-kolalapudi&name=python-natural-language-processing-m3&clip=0&mode=live)\n",
    "\n",
    "* Feature extraction using bag of words model\n",
    "* Use K-means clustering to identify a set of topics\n",
    "* Using K-Nearest Neighbors for classifying text into those topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typical ML Workflow\n",
    "1. Pick your problem\n",
    "  * Identify which type of problem we need to solve\n",
    "2. Represent data\n",
    "  * Represent the data using numeric attributes (features)\n",
    "3. Apply an algorithm\n",
    "  * Use a standard algorithm to find a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Clustering\n",
    "Group items together based on some measure of similarity\n",
    "\n",
    "* Items in a group must be \"similar\" to one another\n",
    "  * Maximize intracluster similarity\n",
    "* Items in different groups must be \"dissimilar\" to one another\n",
    "  * Minimize intercluster similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Represent Data\n",
    "Use meaningful numeric attributes to represent text\n",
    "\n",
    "* Term frequency\n",
    "* TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features\n",
    "Create a list representing the universe of all words that can appear in any text\n",
    "\n",
    "$(w_1, w_2, ..., w_N)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given any piece of text, we can represent it using a tuple of $n$ numbers where each element represents a frequency of one of these words. This is the **Term Frequency Representation**. Note the order of words is lost, hence 'Bag of words' model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Term Frequency - Inverse Document Frequency**: Weight the term frequencies to take the rarity of a word (throughout the corpus) into account.\n",
    "\n",
    "$Weight = 1 / # documents the word appears in$\n",
    "\n",
    "Then, if you have a tuple of word frequencies, weight each word's frequency by the inverse of the number of documents the word appears in. Hence we're multiping the term frequency by idf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${tf} (t,d)=0.5+0.5\\cdot {\\frac {f_{t,d}}{\\max\\{f_{t',d}:t'\\in d\\}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${idf}(t, D) =  \\log \\frac{N}{|\\{d \\in D: t \\in d\\}|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${tfidf} (t,d,D) = {tf} (t,d)\\cdot{idf} (t,D)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means Clustering\n",
    "Documents are represented using TF-IDF\n",
    "\n",
    "Each document is a tuple of $N$ Numbers, where $N$ is the total number of distinct words across all documents, and each tuple of $N$ numbers is a point in an $N$-dimensional hypercube.\n",
    "\n",
    "So imagining our points in an $N$-dimensional space, we can now measure the distance between those points.\n",
    "For clustering, we want to minimize the distance between points within a cluster, and maximize the distance between points in different clusters.\n",
    "\n",
    "For K-Means clustering, we first decide how many clusters we want to divide the data into.\n",
    "  1. Initialize a set of points as the \"K\" Means (Centroids of the clusters you want to find)\n",
    "  2. Assign each point to the cluster belonging to the nearest mean - \n",
    "By taking the average of the coordinates of all the points of one cluster, we can find a new mean/centroid of the clusters.\n",
    "  3. Find the new means/centroids of the clusters\n",
    "\n",
    "Rinse and repeat steps 2 & 3 until we reach convergence (ie the point at which the means don't change anymore). If we can't reach convergence, we may want to set a maximimum number of iterations the algorithm should run before stopping.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "f = open('C:/Users/dclynch/Desktop/test.csv')\n",
    "csv_f = csv.reader(f)\n",
    "\n",
    "testText = []\n",
    "for row in csv_f:\n",
    "    testText+=row\n",
    "    \n",
    "print len(testText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5,min_df=2,stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<90x1169 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 4793 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = vectorizer.fit_transform(testText)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1169 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 56 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 27)\t0.049023091094\n",
      "  (0, 239)\t0.277209207046\n",
      "  (0, 241)\t0.637300184222\n",
      "  (0, 537)\t0.098046182188\n",
      "  (0, 816)\t0.344938297829\n",
      "  (0, 203)\t0.0467321399441\n",
      "  (0, 665)\t0.0370166663892\n",
      "  (0, 800)\t0.049023091094\n",
      "  (0, 957)\t0.195578874933\n",
      "  (0, 621)\t0.0518269886937\n",
      "  (0, 610)\t0.049023091094\n",
      "  (0, 927)\t0.038022438479\n",
      "  (0, 941)\t0.110883682819\n",
      "  (0, 1032)\t0.0467321399441\n",
      "  (0, 607)\t0.0518269886937\n",
      "  (0, 340)\t0.049023091094\n",
      "  (0, 1086)\t0.0352185408793\n",
      "  (0, 376)\t0.038022438479\n",
      "  (0, 999)\t0.207307954775\n",
      "  (0, 843)\t0.110883682819\n",
      "  (0, 240)\t0.147069273282\n",
      "  (0, 377)\t0.0554418414093\n",
      "  (0, 692)\t0.0298475174277\n",
      "  (0, 328)\t0.0554418414093\n",
      "  (0, 844)\t0.221767365637\n",
      "  :\t:\n",
      "  (0, 351)\t0.0554418414093\n",
      "  (0, 818)\t0.098046182188\n",
      "  (0, 944)\t0.0273757654159\n",
      "  (0, 194)\t0.0309906181314\n",
      "  (0, 1010)\t0.0554418414093\n",
      "  (0, 455)\t0.0554418414093\n",
      "  (0, 1111)\t0.0467321399441\n",
      "  (0, 662)\t0.0554418414093\n",
      "  (0, 1040)\t0.140196419832\n",
      "  (0, 824)\t0.0467321399441\n",
      "  (0, 44)\t0.134385505039\n",
      "  (0, 446)\t0.0554418414093\n",
      "  (0, 740)\t0.166325524228\n",
      "  (0, 819)\t0.0832745823891\n",
      "  (0, 633)\t0.0416372911945\n",
      "  (0, 566)\t0.0391157749866\n",
      "  (0, 1063)\t0.0554418414093\n",
      "  (0, 1005)\t0.0554418414093\n",
      "  (0, 678)\t0.0554418414093\n",
      "  (0, 365)\t0.0862345744571\n",
      "  (0, 567)\t0.0467321399441\n",
      "  (0, 286)\t0.0391157749866\n",
      "  (0, 209)\t0.0554418414093\n",
      "  (0, 617)\t0.0554418414093\n",
      "  (0, 323)\t0.049023091094\n"
     ]
    }
   ],
   "source": [
    "print X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 142.042\n",
      "Iteration  1, inertia 75.725\n",
      "Converged at iteration 1: center shift 0.000000e+00 within tolerance 8.124096e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "    n_clusters=6, n_init=1, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters = 6, init = 'k-means++', max_iter = 100, n_init = 1, verbose = True)\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5]), array([15, 18,  9,  8, 17, 23], dtype=int64))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(km.labels_, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text={}\n",
    "for i,cluster in enumerate(km.labels_):\n",
    "    oneDocument = testText[i]\n",
    "    if cluster not in text.keys():\n",
    "        text[cluster] = oneDocument\n",
    "    else:\n",
    "        text[cluster] += oneDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_stopwords = set(stopwords.words('english') + list(punctuation) + [\"million\", \"billion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keywords = {}\n",
    "counts = {}\n",
    "for cluster in range(6):\n",
    "    word_sent = word_tokenize(text[cluster].lower())\n",
    "    word_sent=[word for word in word_sent if word not in _stopwords]\n",
    "    freq = FreqDist(word_sent)\n",
    "    keywords[cluster] = nlargest(100, freq, key=freq.get)\n",
    "    counts[cluster]=freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_keys={}\n",
    "for cluster in range(6):\n",
    "    other_clusters = list(set(range(6))-set([cluster]))\n",
    "    keys_other_clusters = set(keywords[other_clusters[0]]).union(set(keywords[other_clusters[1]]))\n",
    "    unique = set(keywords[cluster])-keys_other_clusters\n",
    "    unique_keys[cluster]=nlargest(20, unique, key=counts[cluster].get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['x2009',\n",
       "  'x2013',\n",
       "  'isolates',\n",
       "  'ili',\n",
       "  'age',\n",
       "  'gentamicin',\n",
       "  'dose',\n",
       "  'group',\n",
       "  'subjects',\n",
       "  'hfrs',\n",
       "  'aki',\n",
       "  'pregnancy',\n",
       "  'igg',\n",
       "  '1',\n",
       "  'rates',\n",
       "  'weeks',\n",
       "  'population',\n",
       "  'susceptibility',\n",
       "  'young',\n",
       "  'ltbi'],\n",
       " 1: ['heart',\n",
       "  'failure',\n",
       "  'hf',\n",
       "  'hfpef',\n",
       "  'fraction',\n",
       "  'ejection',\n",
       "  'cancer',\n",
       "  'hba1c',\n",
       "  'higher',\n",
       "  'preserved',\n",
       "  'ckd',\n",
       "  'methods',\n",
       "  'ef',\n",
       "  'follow-up',\n",
       "  'serum',\n",
       "  'hfref',\n",
       "  'pressure',\n",
       "  'hfnef',\n",
       "  'haemodynamic',\n",
       "  'studies'],\n",
       " 2: ['surgical',\n",
       "  'incontinence',\n",
       "  'fecal',\n",
       "  'techniques',\n",
       "  'anal',\n",
       "  'nerve',\n",
       "  'fi',\n",
       "  'repair',\n",
       "  'stimulation',\n",
       "  'patient',\n",
       "  'option',\n",
       "  'surgeon',\n",
       "  'anaesthetists',\n",
       "  'placement',\n",
       "  'device',\n",
       "  'procedures',\n",
       "  'success',\n",
       "  'anaesthetist',\n",
       "  'options',\n",
       "  'management'],\n",
       " 3: ['pelvic',\n",
       "  'prolapse',\n",
       "  'floor',\n",
       "  'management',\n",
       "  'rectal',\n",
       "  'review',\n",
       "  'posterior',\n",
       "  'crc',\n",
       "  'repair',\n",
       "  'surgical',\n",
       "  'disorders',\n",
       "  'complex',\n",
       "  'approaches',\n",
       "  'organ',\n",
       "  'takotsubo',\n",
       "  'perineal',\n",
       "  'grade',\n",
       "  'pain',\n",
       "  'models',\n",
       "  'approach'],\n",
       " 4: ['care',\n",
       "  'therapy',\n",
       "  'maternal',\n",
       "  'dam',\n",
       "  'rubber',\n",
       "  'new',\n",
       "  'york',\n",
       "  'patient',\n",
       "  'cr',\n",
       "  'burnout',\n",
       "  'use',\n",
       "  'state',\n",
       "  'cces',\n",
       "  'review',\n",
       "  'severe',\n",
       "  'ptsd',\n",
       "  'mindfulness',\n",
       "  'post-traumatic',\n",
       "  'therapist',\n",
       "  'hypertension'],\n",
       " 5: ['maternal',\n",
       "  'vte',\n",
       "  'response',\n",
       "  'presented',\n",
       "  'article',\n",
       "  'death',\n",
       "  '2015',\n",
       "  'editor',\n",
       "  'letter',\n",
       "  'cause',\n",
       "  'hemorrhage',\n",
       "  'united',\n",
       "  'thromboembolism',\n",
       "  'related',\n",
       "  'guidelines',\n",
       "  'states',\n",
       "  'prophylaxis',\n",
       "  'leading',\n",
       "  'severe',\n",
       "  'review']}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
